twitter_data <- readxl::read_excel('C:/Users/Bloomiya/Documents/MyLearning/Twitter Data-20170331T165508Z-001/Twitter Data - Excel/Master - Twitter Data - Views against US The President 2017.xlsx')
write.csv(all_states, file = "allstates.csv")
all_states <- map_data("state")
library(ggplot2)
library(maps
library(ggplot2)
library(maps)
all_states <- map_data("state")
write.csv(all_states, file = "allstates.csv")
orderedstates <- c("AL", "AZ", "AR", "CA", "CO", "CT", "DE", "DC", "FL", "GA", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD", "MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ", "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC", "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY")
write.csv(orderedstates, file = "statesabbs.csv")
all_states$State <- orderedstates
knitr::opts_chunk$set(echo = TRUE)
statenames <- c('Alabama','Arizona','Arkansas','California','Colorado','Connecticut','Delaware','District of Columbia','Florida','Georgia','Idaho','Illinois','Indiana','Iowa','Kansas','Kentucky','Louisiana','Maine','Maryland','Massachusetts','Michigan','Minnesota','Mississippi','Missouri','Montana','Nebraska','Nevada','New Hampshire','New Jersey','New Mexico','New York','North Carolina','North Dakota','Ohio','Oklahoma','Oregon','Pennsylvania','Rhode Island','South Carolina','South Dakota','Tennessee','Texas','Utah','Vermont','Virginia','Washington','West Virginia',
'Wisconsin','Wyoming')
sentiment_score %>% rename(State='StateCode')
library(dplyr)
sentiment_score %>% rename(State='StateCode')
head(tweets_data)
library(dplyr)
# Read data from CSV file
tweets_data = read.csv(file="C:/Users/Bloomiya/Documents/MyLearning/Twitter Data/Tweets - Presidential actions 2017.csv",row.names=NULL,header=TRUE)
# Summarize no of tweets collected per State
tweets_data %>% group_by(State) %>% summarise(tweet_count = n()) %>% arrange(tweet_count)
library('magrittr')
library(stringr)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets_data[,1])
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
clean_tweet = gsub("\\\n", " ", clean_tweet)
clean_tweet <- str_replace_all(clean_tweet,"#[a-z,A-Z]*","")
tweets_data$Clean_Tweet_With_Hashtag <- clean_tweet
# Cleaning round 2 - by removing allwords with hashtags
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets_data[,1])
clean_tweet <- str_replace_all(clean_tweet,"#[a-z,A-Z]*","")
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
clean_tweet = gsub("\\\n", " ", clean_tweet)
tweets_data$Clean_Tweet_Without_Hashtag <- clean_tweet
library(sentimentr)
library(exploratory)
tweets_data$Sentimentscore_withhashtag <- get_sentiment(tweets_data$Clean_Tweet_With_Hashtag)
tweets_data$Sentimentscore_withouthashtag <- get_sentiment(tweets_data$Clean_Tweet_Without_Hashtag)
write.csv(tweets_data, file = "Tweets - Presidential actions 2017 - Sentiment Score.csv")
head(tweets_data)
library(ggplot2)
tweets_data <- tweets_data %>% rename(score_1=`Sentimentscore_withhashtag`, score_2=`Sentimentscore_withouthashtag`)
library(ggplot2)
library(maps)
all_states <- map_data("state")
tweets_data <- tweets_data %>% rename(score_1=`Sentimentscore_withhashtag`, score_2=`Sentimentscore_withouthashtag`)
sentiment_score <- tweets_data %>% group_by(State) %>% summarize(score_2 = mean(score_2))
head(sentiment_score)
sentiment_score %>% rename(State='StateCode')
sentiment_score %>% rename(State="StateCode")
sentiment_score %>% rename(State=StateCode)
sentiment_score %>% rename("State"="StateCode")
sentiment_score %>% rename(StateCode = State)
sentiment_score$State <- statenames
head(sentiment_score)
head(sentiment_score)
sentiment_score <- tweets_data %>% group_by(State) %>% summarize(score_2 = mean(score_2))
head(sentiment_score)
sentiment_score %>% rename(StateCode = State)
head(sentiment_score)
sentiment_score %>% rename(State = StateCode)
library(plyr)
sentiment_score %>% rename(State = 'StateCode')
sentiment_score %>% rename(State = "StateCode")
sentiment_score %>% rename("State" = "StateCode")
sentiment_score %>% plyr::rename("State" = "StateCode")
head(sentiment_score)
state.abb[grep("New York", state.name)]
state.name[grep("NY", state.abb)]
state.name[match(sentiment_score$State, state.abb)]
sentiment_score$StateName <- state.name[match(sentiment_score$State, state.abb)]
sentiment_score <- state.name[match(sentiment_score$State, state.abb)]
head(sentiment_score)
sentiment_score <- tweets_data %>% group_by(State) %>% summarize(score_2 = mean(score_2))
head(sentiment_score)
sentiment_score <- tweets_data %>% group_by(State) %>% summarize(score_2 = mean(score_2))
head(sentiment_score)
sentimentscore <- tweets_data %>% group_by(State) %>% summarize(score_2 = mean(score_2))
sentimentscore
tweets_data
head(tweets_data)
sentimentscore <- tweets_data %>% group_by(State) %>% summarize(score_2 = mean(score_2))
sentimentscore
sentimentscore$State
sentimentscore <- tweets_data %>% dplyr::group_by(State) %>% summarize(score_2 = mean(score_2))
sentimentscore
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
# Read data from CSV file
tweets_data = read.csv(file="C:/Users/Bloomiya/Documents/MyLearning/Twitter Data/Tweets - Presidential actions 2017.csv",row.names=NULL,header=TRUE)
# Summarize no of tweets collected per State
tweets_data %>% group_by(State) %>% summarise(tweet_count = n()) %>% arrange(tweet_count)
library('magrittr')
library(stringr)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets_data[,1])
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
clean_tweet = gsub("\\\n", " ", clean_tweet)
clean_tweet <- str_replace_all(clean_tweet,"#[a-z,A-Z]*","")
tweets_data$Clean_Tweet_With_Hashtag <- clean_tweet
# Cleaning round 2 - by removing allwords with hashtags
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets_data[,1])
clean_tweet <- str_replace_all(clean_tweet,"#[a-z,A-Z]*","")
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
clean_tweet = gsub("\\\n", " ", clean_tweet)
tweets_data$Clean_Tweet_Without_Hashtag <- clean_tweet
knitr::opts_chunk$set(echo = TRUE)
# Load library
library(dplyr)
# Read data from CSV file
tweets_data = readr::read_csv(file="Tweets - Presidential actions 2017.csv")
# See structure of the data
#str(tweets_data)
# Summarize number of tweets collected per State
tweets_data %>% group_by(State) %>% summarise(tweet_count = n()) %>% arrange(tweet_count)
str(tweets_data)
head(tweets_data)
library(dplyr)
# Read data from CSV file
tweets_data = read.csv(file="C:/Users/Bloomiya/Documents/MyLearning/Twitter Data/Tweets - Presidential actions 2017.csv",row.names=NULL,header=TRUE)
# Summarize no of tweets collected per State
tweets_data %>% group_by(State) %>% summarise(tweet_count = n()) %>% arrange(tweet_count)
head(tweets_data)
# Load library
library(dplyr)
# Read data from CSV file
tweets_data = readr::read_csv(file="Tweets - Presidential actions 2017.csv")
# See structure of the data
#head(tweets_data)
# Summarize number of tweets collected per State
tweets_data %>% group_by(State) %>% summarise(tweet_count = n()) %>% arrange(tweet_count)
library('magrittr')
library(stringr)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets_data[,0])
# Load library
library(dplyr)
# Read data from CSV file
tweets_data = readr::read_csv(file="Tweets - Presidential actions 2017.csv")
# See structure of the data
#head(tweets_data)
# Summarize number of tweets collected per State
tweets_data %>% group_by(State) %>% summarise(tweet_count = n()) %>% arrange(tweet_count)
library('magrittr')
library(stringr)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets_data[,0])
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
clean_tweet = gsub("\\\n", " ", clean_tweet)
clean_tweet <- str_replace_all(clean_tweet,"#[a-z,A-Z]*","")
tweets_data$Clean_Tweet_With_Hashtag <- clean_tweet
# Load library
library(dplyr)
# Read data from CSV file
#tweets_data = readr::read_csv(file="Tweets - Presidential actions 2017.csv")
tweets_data = read.csv(file="Tweets - Presidential actions 2017.csv",row.names=NULL,header=TRUE)
# See structure of the data
#head(tweets_data)
# Summarize number of tweets collected per State
tweets_data %>% group_by(State) %>% summarise(tweet_count = n()) %>% arrange(tweet_count)
library('magrittr')
library(stringr)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets_data[,1])
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
clean_tweet = gsub("\\\n", " ", clean_tweet)
clean_tweet <- str_replace_all(clean_tweet,"#[a-z,A-Z]*","")
tweets_data$Clean_Tweet_With_Hashtag <- clean_tweet
# Cleaning round 2 - by removing all words with hashtags
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets_data[,1])
clean_tweet <- str_replace_all(clean_tweet,"#[a-z,A-Z]*","")
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
clean_tweet = gsub("\\\n", " ", clean_tweet)
tweets_data$Clean_Tweet_Without_Hashtag <- clean_tweet
library(sentimentr)
library(exploratory)
tweets_data$Sentimentscore_withhashtag <- get_sentiment(tweets_data$Clean_Tweet_With_Hashtag)
tweets_data$Sentimentscore_withouthashtag <- get_sentiment(tweets_data$Clean_Tweet_Without_Hashtag)
library(ggplot2)
tweets_data <- tweets_data %>% rename(score_1=`Sentimentscore_withhashtag`, score_2=`Sentimentscore_withouthashtag`)
# correlation analysis on two scores
tweets_data %>% select(score_1, score_2) %>% cor()
# State level score distribution analysis
tweets_data %>% group_by(State) %>% summarise(score_1 = mean(score_1)) %>% ggplot(aes(State, score_1)) + geom_bar(stat='identity')
tweets_data %>% group_by(State) %>% summarise(score_1 = mean(score_2)) %>% ggplot(aes(State, score_1)) + geom_bar(stat='identity')
library(ggplot2)
library(maps)
# load map data for US States
all_states <- map_data("state")
# calaculate state level sentiment score
sentiment_score <- tweets_data %>% group_by(State) %>% summarize(score_2 = mean(score_2))
# get state names for each state code
sentiment_score$State <- state.name[match(sentiment_score$State, state.abb)]
# view the dataframe
tbl_df(sentiment_score)
# State name was not retrieved for DC. So it needs to be added seperately
sentiment_score$State[8] <- "District of Columbia"
# map_data() does not return data for Alaska. Hence it needs to be removed from sentiment_score dataframe before it is merged with map_data() results
sentiment_score <- sentiment_score[sentiment_score$State!="Alaska",]
# Since Idaho was missed in the data extraction process it needs to be added to the dataset with a neutral score
sentiment_score <- rbind(sentiment_score, c("Idaho", 0))
sentiment_score <- sentiment_score %>% mutate(score_2 = as.numeric(score_2))
sentiment_score <- sentiment_score[ order(sentiment_score$State), ]
sentiment_score_map <- sentiment_score
sentiment_score_map$State <- sapply(sentiment_score_map$State, tolower)
sentiment_score_map$region <- sentiment_score_map$State
sentiment_score_map <- merge(all_states, sentiment_score_map, by="region")
# plot the data
p <- ggplot()
p <- p + geom_polygon(data=sentiment_score_map, aes(x=long, y=lat, group = group, fill=sentiment_score_map$score_2),colour="white") + scale_fill_continuous(low = "thistle2", high = "darkred", guide="colorbar")
P1 <- p + theme_bw() + labs(fill = "Resistance to 2017 Presidential Actions", title = "2017 Presidential Actions Sentiment Score Analysis", x="", y="")
P1 + scale_y_continuous(breaks=c()) + scale_x_continuous(breaks=c()) + theme(panel.border = element_blank())
# read popular vote election dataset
ElectionResults_data = readr::read_csv(file="2016 National Popular Vote Tracker.csv")
#ElectionResults_data <- ElectionResults_data %>% mutate(VictoryMargin = as.numeric(VictoryMargin)) - assigned NA values to VictoryMargin
ElectionResults_data[,9] <- as.numeric(sub("%","",ElectionResults_data[[9]], fixed = TRUE))
RegressionAnalysis_data <- merge(sentiment_score, ElectionResults_data, by="State")
# perform regression analysis
summary(lm(formula = score_2 ~ VictoryMargin, data=RegressionAnalysis_data))
# plot regression analysis results
ggplotRegression <- function (fit) {
require(ggplot2)
ggplot(fit$model, aes_string(x = names(fit$model)[2], y = names(fit$model)[1])) +
geom_point() +
stat_smooth(method = "lm", col = "red") +
labs(title = paste("Adj R2 = ",signif(summary(fit)$adj.r.squared, 5),
"Intercept =",signif(fit$coef[[1]],5 ),
" Slope =",signif(fit$coef[[2]], 5),
" P =",signif(summary(fit)$coef[2,4], 5)))
}
ggplotRegression(lm(VictoryMargin ~ score_2, data = RegressionAnalysis_data))
install.packages("Hmisc")
library(Hmisc)
cor.test(RegressionAnalysis_data$VictoryMargin, RegressionAnalysis_data$score_2, method="spearman")
ggplotRegression <- function (fit) {
require(ggplot2)
ggplot(fit$model, aes_string(x = names(fit$model)[2], y = names(fit$model)[1])) +
geom_point() +
stat_smooth(method = "lm", col = "red") +
labs(title = paste("Adj R2 = ",signif(summary(fit)$adj.r.squared, 5),
"Intercept =",signif(fit$coef[[1]],5 ),
" Slope =",signif(fit$coef[[2]], 5),
" P =",signif(summary(fit)$coef[2,4], 5)))
}
ggplotRegression(lm(VictoryMargin ~ score_2, data = RegressionAnalysis_data))
sentiment_score %>% rename(Score=`score_2`)
head(sentiment_score)
sentiment_score
sentiment_score <- sentiment_score %>% rename(Score=`score_2`)
head(sentiment_score)
RegressionAnalysis_data <- RegressionAnalysis_data[RegressionAnalysis_data$State!="MT",]
RegressionAnalysis_data <- RegressionAnalysis_data[RegressionAnalysis_data$State!="DC",]
write.csv(RegressionAnalysis_data_final, file = "RegressionAnalysis_data_final")
RegressionAnalysis_data_final <- RegressionAnalysis_data[RegressionAnalysis_data$State!="MT",]
RegressionAnalysis_data_final <- RegressionAnalysis_data[RegressionAnalysis_data$State!="DC",]
write.csv(RegressionAnalysis_data_final, file = "RegressionAnalysis_data_final")
write.csv(RegressionAnalysis_data_final, file = "RegressionAnalysis_data_final")
RegressionAnalysis_data_final <- RegressionAnalysis_data[RegressionAnalysis_data$State!="MT",]
RegressionAnalysis_data_final <- RegressionAnalysis_data[RegressionAnalysis_data$State!="DC",]
write.csv(RegressionAnalysis_data_final, file = "RegressionAnalysis_data_final")
head(RegressionAnalysis_data_final)
tbl_df(RegressionAnalysis_data_final)
tbl_df(sentiment_score)
tbl_df(sentiment_score)
sentiment_score$State <- state.name[match(sentiment_score$State, state.abb)]
sentiment_score$State[8] <- "District of Columbia"
tbl_df(sentiment_score)
tbl_df(sentiment_score)
sentiment_score <- tweets_data %>% group_by(State) %>% summarize(score_2 = mean(score_2))
q()
list.files("dataSets")
getwd()
setwd("C:/Users/Bloomiya/Documents/MyLearning/SpringBoard/Linear Regression - Mini Project/linear_regression")
list.files("dataSets")
states.data <- readRDS("dataSets/states.rds")
states.info <- data.frame(attributes(states.data)[c("names", "var.labels")])
#look at last few labels
tail(states.info, 8)
sts.ex.sat <- subset(states.data, select = c("expense", "csat"))
summary(sts.ex.sat)
# correlation between expense and csat
cor(sts.ex.sat)
plot(sts.ex.sat)
